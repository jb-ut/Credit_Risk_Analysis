# Credit_Risk_Analysis

## Overview
The purpose of this analysis was to predict the risk of credit card fraud for a peer-to-peer lending service company

## Results
### Oversampling
The balanced accuracy score for this model was 0.67, the precision was 0.99 and the recall was 0.67.

![image](https://github.com/jb-ut/Credit_Risk_Analysis/blob/main/oversampling_1.PNG)
![image](https://github.com/jb-ut/Credit_Risk_Analysis/blob/main/oversampling_2.PNG)

### SMOTE Oversampling
The balanced accuracy score for this model was 0.66, the precision was 0.99 and the recall was 0.66.

![image](https://github.com/jb-ut/Credit_Risk_Analysis/blob/main/SMOTE-oversampling_1.PNG)
![image](https://github.com/jb-ut/Credit_Risk_Analysis/blob/main/SMOTE-oversampling_2.PNG)

### Undersampling
The balanced accuracy score for this model was 0.42, the precision was 0.99 and the recall was 0.42.

![image](https://github.com/jb-ut/Credit_Risk_Analysis/blob/main/undersampling_1.PNG)
![image](https://github.com/jb-ut/Credit_Risk_Analysis/blob/main/undersampling_2.PNG)

### Over and Under Sampling
The balanced accuracy score for this model was 0.55, the precision was 0.99 and the recall was 0.55.

![image](https://github.com/jb-ut/Credit_Risk_Analysis/blob/main/over-under-1.PNG)
![image](https://github.com/jb-ut/Credit_Risk_Analysis/blob/main/over-under-2.PNG)

### Balanced Random Forest Classifier
The balanced accuracy score for this model was 0.78, the precision was 0.99 and the recall was 0.87.

![image](https://github.com/jb-ut/Credit_Risk_Analysis/blob/main/balanced-1.PNG)
![image](https://github.com/jb-ut/Credit_Risk_Analysis/blob/main/balanced-2.PNG)

### Easy Ensemble AdaBoost Classifier
The balanced accuracy score for this model was 0.93, the precision was 0.99 and the recall was 0.94.

![image](https://github.com/jb-ut/Credit_Risk_Analysis/blob/main/easy-ad-1.PNG)
![image](https://github.com/jb-ut/Credit_Risk_Analysis/blob/main/easy-ad-2.PNG)

## Summary

I used six models the boosted Easy Ensemble AdaBooster Classifier provided the most confidence. Models that provided the best precsion lead to less false positives. 



